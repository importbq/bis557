sim = length(Time) - 1
means[j] = sim
}
print(mean(means))
break
}
}
# for (b). let alpha = 1, beta = 2 and a = 8
lambda = function(t){
if (t < 8){
return(1)
}
else{
return(2)
}
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
#calculate the simulated mean:
simulation(seed = 120)
# for (b). let alpha = 1, beta = 2 and a = 8
lambda = function(t){
exp(t - 10)
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
#calculate the simulated mean:
simulation(seed = 240)
# for (b). let alpha = 1, beta = 2 and a = 8
lambda = function(t){
exp(t - 10)
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
#calculate the simulated mean:
simulation(seed = 240)
# for (b). let alpha = 1, beta = 2 and a = 8
lambda = function(t){
exp(t - 10)
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
#calculate the simulated mean:
simulation(seed = 240)
?rbinom
rbinom(1,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
rbinom(1,size=2,prob=0.5)
hist(rbinom(100,size=2,prob=0.5))
hist(rbinom(100,size=1,prob=0.5))
lambda = 5
alpha = 0.7
yi = rbinom(n,size = 1, p = alpha + ((1-alpha)*exp(-lambda)))
n = 50
yi = rbinom(n,size = 1, p = alpha + ((1-alpha)*exp(-lambda)))
alpha + ((1-alpha)*exp(-lambda))
hist(yi)
yi[yi!=0] = rpois(1,lambda)
yi
rpois(1,lambda)
yi = rbinom(n,size = 1, p = 1- alpha - ((1-alpha)*exp(-lambda)))
yi[yi != 0] = sapply(yi[yi != 0], function(x){rpois(x,lambda=lambda)})
print(yi)
hist(yi)
rzip = function(n, lambda, alpha){
#fixed probability for being 0
yi = rbinom(n,size = 1, p = 1- alpha - ((1-alpha)*exp(-lambda)))
#replace each yi that is not 0 with poisson randomized number:
yi[yi != 0] = sapply(yi[yi != 0], function(x){rpois(x,lambda=lambda)})
return(yi)
}
observation = rzip(1000,5,0.7)
hist(observation,bin=1)
hist(observation,breaks=1)
hist(observation,breaks=1)
hist(observation)
?hist
hist(observation,breaks = 15)
hist(observation,breaks = 20)
hist(observation,breaks = 15)
sum(yi[yi != 0])/length(yi[yi != 0])
length(yi[yi != 0])
1/13
length(observation[observation > 0])
50* 0.3
n = length(yi)
m = length(yi[yi==0])
alpha = 1
lambda = 1
lambda_deriv =  function (alpha, lambda){
((m/(alpha + (1-alpha)*exp(-lambda)))*(1-alpha)*exp(-lambda)) + n - m - (sum_y/lambda)
}
lambda_deriv(1,1)
sum_y = sum(Yi)
sum_y = sum(yi)
lambda_deriv(1,1)
factorial(0)
sum(sapply(yi,factorial()))
sum(sapply(yi,factorial)
)
prod(sapply(yi,factorial))
for (i in 1:100){
w = lambda_deriv(alpha = alpha, lambda = lambda)
b = alpha_deriv(alpha = alpha, lambda = lambda)
lambda = lambda - lr * w
alpha = alpha - lr * b
if (w < 0.01 & b < 0.01){
break
}
print(paste('lambda is:', w, 'alpha is:', b))
}
alpha = 1
lambda = 1
lambda_deriv =  function (alpha, lambda){
((m/(alpha + (1-alpha)*exp(-lambda)))*(1-alpha)*exp(-lambda)) + n - m - (sum_y/lambda)
}
alpha_deriv = function (alpha, lambda){
n - m - ((m*(1-alpha)*(1-exp(-lambda)))/(alpha + ((1-alpha)*exp(-lambda))))
}
for (i in 1:100){
w = lambda_deriv(alpha = alpha, lambda = lambda)
b = alpha_deriv(alpha = alpha, lambda = lambda)
lambda = lambda - lr * w
alpha = alpha - lr * b
if (w < 0.01 & b < 0.01){
break
}
print(paste('lambda is:', w, 'alpha is:', b))
}
lr = 0.01
for (i in 1:100){
w = lambda_deriv(alpha = alpha, lambda = lambda)
b = alpha_deriv(alpha = alpha, lambda = lambda)
lambda = lambda - lr * w
alpha = alpha - lr * b
if (w < 0.01 & b < 0.01){
break
}
print(paste('lambda is:', w, 'alpha is:', b))
}
lr = 0.01
for (i in 1:100){
w = lambda_deriv(alpha = alpha, lambda = lambda)
b = alpha_deriv(alpha = alpha, lambda = lambda)
lambda = lambda - lr * w
alpha = alpha - lr * b
if (abs(w) < 0.01 & abs(b) < 0.01){
break
}
print(paste('lambda is:', w, 'alpha is:', b))
}
lr = 0.01
while(TRUE){
w = lambda_deriv(alpha = alpha, lambda = lambda)
b = alpha_deriv(alpha = alpha, lambda = lambda)
lambda = lambda - lr * w
alpha = alpha - lr * b
if (abs(w) < 0.01 & abs(b) < 0.01){
break
}
print(paste('lambda is:', w, 'alpha is:', b))
}
grad_desc = function (yi, lr = 0.01, prt = TRUE){
n = length(yi)
m = length(yi[yi==0])
sum_y = sum(yi)
# initialize:
alpha = 1
lambda = 1
# create functions for lambda and alpha derivatives:
lambda_deriv =  function (alpha, lambda){
((m/(alpha + (1-alpha)*exp(-lambda)))*(1-alpha)*exp(-lambda)) + n -
m -  (sum_y/lambda)
}
alpha_deriv = function (alpha, lambda){
n - m - ((m*(1-alpha)*(1-exp(-lambda)))/(alpha + ((1-alpha)*exp(-lambda))))
}
# likelihood too large! we only use derivative as criterion
#loss = function (alpha, lambda){
#  los = (m * log(alpha + (1- alpha)*exp(-lambda))) + (n - m)*log(1-alpha) -                   ((n-m)*lambda) +
#    sum_y * log(lambda) - log(prod(sapply(yi,factorial)))
#}
while(TRUE){
w = lambda_deriv(alpha = alpha, lambda = lambda)
b = alpha_deriv(alpha = alpha, lambda = lambda)
lambda = lambda - lr * w
alpha = alpha - lr * b
if (abs(w) < 0.01 & abs(b) < 0.01){
print(paste('found alpha: ', alpha))
print(paste('found lambda: ', lambda))
}
# monitor progress
if (prt == TRUE){print(paste('lambda is:', w, 'alpha is:', b))}
}
}
grad_desc(yi=yi)
#we use gradient descent to find the maximum of the log likelihood function:
grad_desc = function (yi, lr = 0.01, prt = TRUE){
n = length(yi)
m = length(yi[yi==0])
sum_y = sum(yi)
# initialize:
alpha = 1
lambda = 1
# create functions for lambda and alpha derivatives:
lambda_deriv =  function (alpha, lambda){
((m/(alpha + (1-alpha)*exp(-lambda)))*(1-alpha)*exp(-lambda)) + n -
m -  (sum_y/lambda)
}
alpha_deriv = function (alpha, lambda){
n - m - ((m*(1-alpha)*(1-exp(-lambda)))/(alpha + ((1-alpha)*exp(-lambda))))
}
# likelihood too large! we only use derivative as criterion
#loss = function (alpha, lambda){
#  los = (m * log(alpha + (1- alpha)*exp(-lambda))) + (n - m)*log(1-alpha) -                   ((n-m)*lambda) +
#    sum_y * log(lambda) - log(prod(sapply(yi,factorial)))
#}
while(TRUE){
w = lambda_deriv(alpha = alpha, lambda = lambda)
b = alpha_deriv(alpha = alpha, lambda = lambda)
lambda = lambda - lr * w
alpha = alpha - lr * b
if (abs(w) < 0.01 & abs(b) < 0.01){
print(paste('found alpha: ', alpha))
print(paste('found lambda: ', lambda))
break
}
# monitor progress
if (prt == TRUE){print(paste('lambda is:', w, 'alpha is:', b))}
}
}
grad_desc(yi = yi)
mle_zip(yi = observation)
mle_zip= function (yi, lr = 0.01, prt = TRUE){
n = length(yi)
m = length(yi[yi==0])
sum_y = sum(yi)
# initialize:
alpha = 1
lambda = 1
# create functions for lambda and alpha derivatives:
lambda_deriv =  function (alpha, lambda){
((m/(alpha + (1-alpha)*exp(-lambda)))*(1-alpha)*exp(-lambda)) + n -
m -  (sum_y/lambda)
}
alpha_deriv = function (alpha, lambda){
n - m - ((m*(1-alpha)*(1-exp(-lambda)))/(alpha + ((1-alpha)*exp(-lambda))))
}
# likelihood too large! we only use derivative as criterion
#loss = function (alpha, lambda){
#  los = (m * log(alpha + (1- alpha)*exp(-lambda))) + (n - m)*log(1-alpha) -                   ((n-m)*lambda) +
#    sum_y * log(lambda) - log(prod(sapply(yi,factorial)))
#}
while(TRUE){
w = lambda_deriv(alpha = alpha, lambda = lambda)
b = alpha_deriv(alpha = alpha, lambda = lambda)
lambda = lambda - lr * w
alpha = alpha - lr * b
if (abs(w) < 0.01 & abs(b) < 0.01){
print(paste('found alpha: ', alpha))
print(paste('found lambda: ', lambda))
break
}
# monitor progress
if (prt == TRUE){print(paste('lambda is:', w, 'alpha is:', b))}
}
}
mle_zip(yi = observation)
#we use gradient descent to find the maximum of the log likelihood function:
mle_zip= function (yi, lr = 0.01, prt = TRUE){
n = length(yi)
m = length(yi[yi==0])
sum_y = sum(yi)
# initialize:
alpha = 1
lambda = 1
# create functions for lambda and alpha derivatives:
lambda_deriv =  function (alpha, lambda){
((m/(alpha + (1-alpha)*exp(-lambda)))*(1-alpha)*exp(-lambda)) + n -
m -  (sum_y/lambda)
}
alpha_deriv = function (alpha, lambda){
n - m - ((m*(1-alpha)*(1-exp(-lambda)))/(alpha + ((1-alpha)*exp(-lambda))))
}
# likelihood too large! we only use derivative as criterion
#loss = function (alpha, lambda){
#  los = (m * log(alpha + (1- alpha)*exp(-lambda))) + (n - m)*log(1-alpha) -                   ((n-m)*lambda) +
#    sum_y * log(lambda) - log(prod(sapply(yi,factorial)))
#}
while(TRUE){
w = lambda_deriv(alpha = alpha, lambda = lambda)
b = alpha_deriv(alpha = alpha, lambda = lambda)
lambda = lambda - lr * w
alpha = alpha - ((lr * 10) * b)
if (abs(w) < 0.01 & abs(b) < 0.01){
print(paste('found alpha: ', alpha))
print(paste('found lambda: ', lambda))
break
}
# monitor progress
if (prt == TRUE){print(paste('lambda is:', w, 'alpha is:', b))}
}
}
# test this function in the simulated data:
mle_zip(yi = observation)
#we use gradient descent to find the maximum of the log likelihood function:
mle_zip= function (yi, lr = 0.01, prt = TRUE){
n = length(yi)
m = length(yi[yi==0])
sum_y = sum(yi)
# initialize:
alpha = 1
lambda = 1
# create functions for lambda and alpha derivatives:
lambda_deriv =  function (alpha, lambda){
((m/(alpha + (1-alpha)*exp(-lambda)))*(1-alpha)*exp(-lambda)) + n -
m -  (sum_y/lambda)
}
alpha_deriv = function (alpha, lambda){
n - m - ((m*(1-alpha)*(1-exp(-lambda)))/(alpha + ((1-alpha)*exp(-lambda))))
}
# likelihood too large! we only use derivative as criterion
#loss = function (alpha, lambda){
#  los = (m * log(alpha + (1- alpha)*exp(-lambda))) + (n - m)*log(1-alpha) -                   ((n-m)*lambda) +
#    sum_y * log(lambda) - log(prod(sapply(yi,factorial)))
#}
while(TRUE){
w = lambda_deriv(alpha = alpha, lambda = lambda)
b = alpha_deriv(alpha = alpha, lambda = lambda)
lambda = lambda - lr * w
alpha = alpha - ((lr * 100) * b)
if (abs(w) < 0.01 & abs(b) < 0.01){
print(paste('found alpha: ', alpha))
print(paste('found lambda: ', lambda))
break
}
# monitor progress
if (prt == TRUE){print(paste('lambda is:', w, 'alpha is:', b))}
}
}
# test this function in the simulated data:
mle_zip(yi = observation)
# Newton's updates to find the inverse:
generate_ti = function(lambda, Ti){ # Ti is from previously generated t
# integrate for Lambda:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
# find the inverse for w
w = 1
u = runif(1)
g_prime = -lambda(w)
g = -log(u) - Lambda(Ti + w) + Lambda(Ti)
w_new = w - (g/g_prime)
if ((abs(w_new - w) < 10^(-4)) | (abs(g) < 10^(-4))) {
w = w_new
break
}
w = w_new
return(w)
}
# Run the simulations and calculate the means based on 10 simulations. set run = True to run and change seed to see the mean changes.
simulation = function(run = TRUE, seed){
set.seed(seed)
while(run){
means = numeric(0)
for (j in 1:10){ #simulate 10 times to find means
# initialize
Time = 0
i = 1
while(TRUE){
#counter, first is 0
w = generate_ti(lambda = lambda, Ti = Time[i])
update = Time[i] + w
if (update < 10){ # check to see if exceed the upper limit
i = i + 1
Time[i] = update
}
else{
break
}
}
sim = length(Time) - 1
means[j] = sim
}
print(mean(means))
break
}
}
lambda = function(t){
return(10*exp-3*(t))
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
lambda = function(t){
return(10*exp(-3*(t)))
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
lambda = function(t){
return(0.2*exp(-3*(t)))
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
lambda = function(t){
return(20*exp(-3*(t)))
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
lambda = function(t){
return(20*exp(-4*(t)))
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
lambda = function(t){
return(20*exp(-10*(t)))
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
lambda = function(t){
return(20*exp(-0.1*(t)))
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
lambda = function(t){
return(20*exp(-(t)))
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
simulation(seed = 240)
lambda = function(t){
return(10*exp(-(t)))
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
simulation(seed = 240)
lambda = function(t){
return(10*exp(-2(t)))
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
lambda = function(t){
return(10*exp(-2(t)))
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
lambda = function(t){
return(10*exp(-2*(t)))
}
# calculate the mean:
Lambda =function(t){
Lambda = integrate(Vectorize(lambda),lower=0,upper=t)$value
return(Lambda)}
print(Lambda(10))
simulation(seed = 240)
