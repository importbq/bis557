}
}
# if we set t = 0.5, E(Y1(0.5)) is estimated:
print(sum(y1_traject < 0.5)/1000)
#solution we get from part b:
e_t = function(t){
k1 = (((((2*a*b^2+2*a^2*b)*t-b^2-2*a*b)*exp(2*a*t)+b^2+2*a*b+a^2)*exp(2*b*t)-a^2)*exp(-2*b*t-2*a*t))/(2*a*b*(b+a))
k2 = (1 - exp(-2*a*t))
return(k1+k2)
}
e_t(0.5)
a = alpha = 0.5
b = beta = 0.5
lambda = function(Y){
alpha + beta * Y
}
Y1 = 0
Y2 = 0
# record timing for y1 and y2
set.seed(120)
y1_traject = 0
y2_traject = 0
for (i in 1:1000  ){
t = rexp(1, rate = lambda(Y1) + lambda(Y2))
#multinomical distribution for either Y1 or Y2
md = rmultinom(1, 1, c(lambda(Y1),lambda(Y2)))
if (md[1] == 1){
Y1 = 1
tnew = rexp(1, rate = lambda(1))
y1_traject[i] = t
y2_traject[i] = t + tnew
}
if (md[2] == 1){
Y2 = 1
tnew = rexp(1, rate = lambda(1))
y2_traject[i] = t
y1_traject[i] = t + tnew
}
}
# if we set t = 0.5, E(Y1(0.5)) is estimated:
print(sum(y1_traject < 0.5)/1000)
#solution we get from part b:
e_t = function(t){
k1 = (((((2*a*b^2+2*a^2*b)*t-b^2-2*a*b)*exp(2*a*t)+b^2+2*a*b+a^2)*exp(2*b*t)-a^2)*exp(-2*b*t-2*a*t))/(2*a*b*(b+a))
k2 = (1 - exp(-2*a*t))
return(k1+k2)
}
e_t(0.5)
a = alpha = 0.5
b = beta = 0.5
lambda = function(Y){
alpha + beta * Y
}
Y1 = 0
Y2 = 0
# record timing for y1 and y2
set.seed(200)
y1_traject = 0
y2_traject = 0
for (i in 1:1000  ){
t = rexp(1, rate = lambda(Y1) + lambda(Y2))
#multinomical distribution for either Y1 or Y2
md = rmultinom(1, 1, c(lambda(Y1),lambda(Y2)))
if (md[1] == 1){
Y1 = 1
tnew = rexp(1, rate = lambda(1))
y1_traject[i] = t
y2_traject[i] = t + tnew
}
if (md[2] == 1){
Y2 = 1
tnew = rexp(1, rate = lambda(1))
y2_traject[i] = t
y1_traject[i] = t + tnew
}
}
# if we set t = 0.5, E(Y1(0.5)) is estimated:
print(sum(y1_traject < 0.5)/1000)
#solution we get from part b:
e_t = function(t){
k1 = (((((2*a*b^2+2*a^2*b)*t-b^2-2*a*b)*exp(2*a*t)+b^2+2*a*b+a^2)*exp(2*b*t)-a^2)*exp(-2*b*t-2*a*t))/(2*a*b*(b+a))
k2 = (1 - exp(-2*a*t))
return(k1+k2)
}
e_t(0.5)
knitr::include_graphics("/Users/bo/Desktop/2020 fall/Stochastic/HW4/Screen Shot 2020-11-03 at 2.13.43 AM.png")
T_full = list()
X_full = list()
for (i in 1:100){
# Hyperparameters:
tmax = 10
N = 100
c = 10
beta = 0.1
alpha = 0.8
u = 0.3
# start with 1 infected and 99 uninfected
infect = 1
SUS = 99
# at hospital:
X = 0
t_traject = 0
X_traject = 0
#immume:
immune = 0
while (max(t_traject) < tmax){
#simulate a competing event time
go_hospital = alpha * infect
got_virus = beta * infect * (N - infect - X - immune)
leave_hospital = min(X,c) * u
event_rate = go_hospital + got_virus + leave_hospital
event_time = rexp(1,event_rate)
#stop if infect and X are both 0
if (infect == 0 & X == 0){
break
}
#multinomical distribution for the three events
md = rmultinom(1, 1, c(go_hospital, got_virus, leave_hospital))
# patient goes to the hospital
if (md[1] == 1){
infect = infect - 1
X = X + 1
X_traject = c(X_traject, X)
t_traject = c(t_traject,(max(t_traject)+ event_time))
}
# patient infects
if (md[2] == 1){
infect = infect + 1
SUS = SUS - 1
}
# patient leaves the hospital
if (md[3] == 1){
X = X - 1
immune = immune + 1
X_traject = c(X_traject, X)
t_traject = c(t_traject,(max(t_traject)+ event_time))
}
}
T_full[[length(T_full)+1]] = t_traject
X_full[[length(X_full)+1]] = X_traject
}
plot(T_full[[1]],X_full[[1]],type='l',ylim = c(0,100),xlab = "Time",ylab = "X(t)")
for (i in (2:100)){
lines(T_full[[i]],X_full[[i]])
}
# turn the previous simulation into a function:
sim = function(c){
# Hyperparameters:
tmax = 10
N = 100
beta = 0.1
alpha = 0.8
u = 0.3
# start with 1 infected and 99 uninfected
infect = 1
SUS = 99
# at hospital:
X = 0
t_traject = 0
X_traject = 0
#immume:
immune = 0
while (max(t_traject) < tmax){
#simulate a competing event time
go_hospital = alpha * infect
got_virus = beta * infect * (N - infect - X - immune)
leave_hospital = min(X,c) * u
event_rate = go_hospital + got_virus + leave_hospital
event_time = rexp(1,event_rate)
#stop if infect and X are both 0
if (infect == 0 & X == 0){
break
}
#multinomical distribution for the three events
md = rmultinom(1, 1, c(go_hospital, got_virus, leave_hospital))
# patient goes to the hospital
if (md[1] == 1){
infect = infect - 1
X = X + 1
X_traject = c(X_traject, X)
t_traject = c(t_traject,(max(t_traject)+ event_time))
}
# patient infects
else if (md[2] == 1){
infect = infect + 1
SUS = SUS - 1
}
# patient leaves the hospital
else if (md[3] == 1){
X = X - 1
immune = immune + 1
X_traject = c(X_traject, X)
t_traject = c(t_traject,(max(t_traject)+ event_time))
}
}
# calculate the portion that X_traject is less than 50
return(sum((X_traject - 10) < 50)/length(X_traject))
}
#based on experience c should fall within this range
set.seed(100)
for (c in (20:100)){
rate = 0
for (i in (1:10)){
rate[i] = sim(c)
}
k = sum(rate)/length(rate)
print(paste("C is ",c, ", Pr(X(t) < 50) = ", k))
}
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1119.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1119.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1119.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1119.jpg')
include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/Screen Shot 2020-11-03 at 2.20.43 AM.png')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1119.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/Screen Shot 2020-11-03 at 2.20.43 AM.png')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1119.jpg','/Users/bo/Desktop/2020 fall/Stochastic/HW4/Screen Shot 2020-11-03 at 2.20.43 AM.png')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1119.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/Screen Shot 2020-11-03 at 2.20.43 AM.png')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1120.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1120.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1120.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1121.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1120.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1121.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1122.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/Screen Shot 2020-11-03 at 2.14.47 AM.png')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/IMG_1122.jpg')
knitr::include_graphics('/Users/bo/Desktop/2020 fall/Stochastic/HW4/Screen Shot 2020-11-03 at 2.14.47 AM.png')
set.seed(100)
alpha = 0.5
beta = 1
d = runif(100)
lambda = alpha + beta * d
# make the max time 5 hours and simulate 100 times
tr = pbtree(b = lambda[1],t = 5,type = "continuous")
for (i in 2:100){
new = pbtree(b = lambda[i],t = 5,type = "continuous")
tr = c(tr,new)
}
#plot the X(t) against t
ltt(tr,log.lineages = FALSE)
yule(tr[[2]])
lamb = (0)
for (i in 1:100){
k = yule(tr[[i]])
lamb[i] = k$lambda
}
lamb
log_lamb = log(lamb)
fit = lm(log_lamb ~ d)
fit$coefficients
set.seed(100)
alpha = 0.5
beta = 1
d = runif(100)
lambda = exp(alpha + beta * d)
# make the max time 5 hours and simulate 100 times
tr = pbtree(b = lambda[1],t = 5,type = "continuous")
lambda = exp(alpha + beta * d)
lambda = alpha + beta * d
set.seed(100)
alpha = 0.2
beta = 0.5
d = runif(100)
lambda = exp(alpha + beta * d)
# make the max time 3 hours and simulate 100 times
tr = pbtree(b = lambda[1],t = 3,type = "continuous")
for (i in 2:100){
new = pbtree(b = lambda[i],t = 3,type = "continuous")
tr = c(tr,new)
}
#plot the X(t) against t
ltt(tr,log.lineages = FALSE)
# MLE does not work in my case so I tried to use build-in function to estimate alpha and beta.
# Yule function gives mle for lambda, for all the samples we find their MLE lambdas:
lamb = (0)
for (i in 1:100){
k = yule(tr[[i]])
lamb[i] = k$lambda
}
# we use the lambdas to find the MLE for alpha and beta:
log_lamb = log(lamb)
fit = lm(log_lamb ~ d)
fit$coefficients
knitr::opts_chunk$set(echo = TRUE)
test = read.table('/Users/bo/Desktop/2020 fall/Machine learning/HW2/test', sep = ",", header = T)
#read in the data:
train = read.table('/Users/bo/Desktop/2020 fall/Machine learning/HW2/training', sep = ",", header = T)
test = read.table('/Users/bo/Desktop/2020 fall/Machine learning/HW2/test', sep = ",", header = T)
View(test)
drop(train$row.names)
View(train)
train = drop(train$row.names)
test$row.names = NULL
#read in the data:
train = read.table('/Users/bo/Desktop/2020 fall/Machine learning/HW2/training', sep = ",", header = T)
test = read.table('/Users/bo/Desktop/2020 fall/Machine learning/HW2/test', sep = ",", header = T)
train$row.names = NULL
test$row.names = NULL
View(test)
library(mgcv)
View(train)
View(test)
ga = gam(y~.,family = multinom(K = 10))
ga = gam(y~.,data = train,family = multinom(K = 10))
ga = gam(form = y~.,data = train,family = multinom(K = 10))
form <- paste0("y ~ ", paste0("x.", seq(1, 10, 1), collapse = "+"))
ga = gam(form,data = train,family = multinom(K = 10))
library(earth)
install.packages('earth')
library(earth)
mars1 <- earth(
y ~ .,
data = train
)
View(test)
pred = predict(mars1,newdata = test[,2:11])
View(pred)
View(pred)
View(train)
View(test)
View(pred)
View(test)
View(pred)
View(test)
View(mars1)
View(train)
train$y = as.factor(train$y)
View(train)
mars1 <- earth(
y ~ .,
data = train
)
pred = predict(mars1,newdata = test[,2:11])
View(pred)
pred_f = apply(pred,which.max)
pred_f = apply(pred,FUN = which.max)
pred_f = apply(pred,1,FUN = which.max)
sum(pred_f != test$y)
library(earth)
train$y = as.factor(train$y)
mars1 <- earth(
y ~ .,
data = train
)
pred = predict(mars1,newdata = test[,2:11])
pred_f = apply(pred,1,FUN = which.max)
print(paste("accuracy for MARS is: ",sum(pred_f != test$y)/length(test$y)))
power.prop.test(p1 = 0.2,p2 = 0.1, sig.level = 0.05,power = 0.9)
42/64
power.prop.test(p1 = 0.5,p2 = 0.0.65625, sig.level = 0.05,power = 0.9)
power.prop.test(p1 = 0.5,p2 = 0.65625, sig.level = 0.05,power = 0.9)
power.prop.test(p1 = 0.5,p2 = 0.25, sig.level = 0.05,power = 0.9)
print(power.prop.test(p1 = 0.5,p2 = 0.25, sig.level = 0.05,power = 0.9))
# assume average 90% loss to follow up
pwr.t2n.test(n1 = 77, n2 = 77*0.9, sig.level=.05, power = .90)
library(pwr)
install.packages("pwr")
library(pwr)
pwr.t2n.test(n1 = 77, n2 = 77*0.9, sig.level=.05, power = .90)
# assume average 90% loss to follow up
print(pwr.t2n.test(n1 = 77, n2 = 77*0.9, sig.level=.05, power = .90))
sig = 2.1
print((sig * 0.5403459))
# assume average 90% loss to follow up
print(pwr.t2n.test(n1 = 79, n2 = 79*0.9, sig.level=.05, power = .90))
sig = 2.1
print((sig * 0.5403459))
# assume average 90% loss to follow up
print(pwr.t2n.test(n1 = 77, n2 = 77*0.9, sig.level=.05, power = .90))
sig = 2.1
print((sig * 0.5403459))
# assume average 90% loss to follow up
print(pwr.t2n.test(n1 = 77, n2 = 77*0.9, sig.level=.05, power = .90))
sig = 2.1^2
print((sig * 0.5403459))
# Two sample proportion
power.prop.test(p1 = 0.5,p2 = 0.25, sig.level = 0.025,power = 0.9)
# Two sample proportions after bonferroni adjustment
print(power.prop.test(p1 = 0.5,p2 = 0.25, sig.level = 0.025,power = 0.9))
print(91 * 2)
91*0.9
ceiling(91*0.9)
91*0.9
# assume average 90% loss to follow up
print(pwr.t2n.test(n1 = ceiling(91*0.9), n2 = ceiling(91*0.9), sig.level=.025, power = .90))
library(pwr)
# assume average 90% loss to follow up
print(pwr.t2n.test(n1 = ceiling(91*0.9), n2 = ceiling(91*0.9), sig.level=.025, power = .90))
sig = 2.1^2
print((sig * 0.5403459))
# assume average 90% loss to follow up
print(pwr.t2n.test(n1 = ceiling(91*0.9), n2 = ceiling(91*0.9), sig.level=.025, power = .90))
sig = 2.1^2
print((sig * 0.5545017))
112*0.9
112/(1.1)
91/(0.9)
91/(0.9)
79/(0.85)
91/0.81
# assume average 90% loss to follow up
# 2 sample t-test
print(pwr.t2n.test(n1 = ceiling(91*0.9), n2 = ceiling(91*0.9), sig.level=.025, power = .90))
sig = 2.1^2
print((sig * 0.5545017))
# Two sample proportions after bonferroni adjustment
print(power.prop.test(p1 = 0.5,p2 = 0.25, sig.level = 0.025,power = 0.9))
print(91 * 2)
# assume average 90% loss to follow up
# 2 sample t-test
print(pwr.t2n.test(n1 = 77, n2 = 77, sig.level=.05, power = .90))
library(pwr)
# assume average 90% loss to follow up
# 2 sample t-test
print(pwr.t2n.test(n1 = 77, n2 = 77, sig.level=.05, power = .90))
sig = 2.1^2
print((sig * 0.5545017))
sqrt(2*2.1^2*(1.96+1.28)^2/77)
# assume average 90% loss to follow up
# 2 sample t-test
print(pwr.t2n.test(n1 = ceiling(91*0.9), n2 = ceiling(91*0.9), sig.level=.025, power = .90))
library(pwr)
# assume average 90% loss to follow up
# 2 sample t-test
print(pwr.t2n.test(n1 = ceiling(91*0.9), n2 = ceiling(91*0.9), sig.level=.025, power = .90))
sig = 2.1^2
print((sig * 0.5545017))
(21/28)/(14/28)
sqrt(1/21 - 1/(28) + 1/14 - 1/28)
sqrt(log(1/21 - 1/(28) + 1/14 - 1/28))
sqrt((1/21) - (1/(28)) + (1/14) - (1/28))
qnorm(0.975)
log(1.5) + 1.96 * 0.218
log(1.5) + (1.96 * 0.218)
log(1.5) - (1.96 * 0.218)
rr = function(a,b,c,d){
(a/(a+b))/(c/(c+d))
}
rr(315,285,210,390)
log(1.5) + (1.96 * 0.0678)
log(1.5) + (1.96 * 0.0678)
log(1.5) - (1.96 * 0.0678)
chisq.test(as.matrix(c(21,14),c(7,14)))
chisq.test(as.matrix(c(21,14),c(7,14)))
chisq.test(as.matrix(c(30,20),c(10,20)))
exp(0.273)
exp(0.538)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(reticulate)
library(bis557)
use_python("/Users/bo/.pyenv/versions/3.7.3/lib/python3.7")
reticulate::source_python("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/ridge.py")
Y
library(reticulate)
source('/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/HW4.R')
use_python("/Users/bo/.pyenv/versions/3.7.3/lib/python3.7")
reticulate::source_python("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/ridge.py")
reticulate::source_python("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/batch.py")
reticulate::repl_python()
library(reticulate)
py_install("sklearn")
reticulate::repl_python()
py_install('sklearn')
df = as.data.frame(cbind(X,Y))
reticulate::repl_python()
df = as.data.frame(cbind(X,Y))
beta = c(0.2,-0.2 ,0.1 ,0.5)
X = matrix(rnorm(50000*4), nrow=10000, ncol = 4)
Y = X %*% beta + rnorm(50000)
beta = c(0.2,-0.2 ,0.1 ,0.5)
X = matrix(rnorm(50000*4), nrow=50000, ncol = 4)
Y = X %*% beta + rnorm(50000)
df = as.data.frame(cbind(X,Y))
View(df)
fit = lm(V5 ~., df)
View(fit)
print(fit$coefficients)
# create dataset:
beta = c(0.2,-0.2 ,0.1 ,0.5)
X = matrix(rnorm(1000*4), nrow=1000, ncol = 4)
# we introduce colinearity into the dataset
X[,1] = 2*X[,2] + X[,1]
y = X %*% beta + rnorm(1000) #add noise
# run ridge regression:
ridge_hw4(X,y,lambda = 1)
df = as.data.frame(cbind(X,y))
fit = lm(V5 ~., df)
print(fit$coefficients)
install.packages('casl')
library(casl)
install.packages('casl')
library('glmnet')
install.packages('casl')
library(reticulate)
library(glmnet)
source('/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/HW4.R')
use_python("/Users/bo/.pyenv/versions/3.7.3/lib/python3.7")
reticulate::source_python("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/ridge.py")
reticulate::source_python("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/batch.py")
reticulate::source_python("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/lasso.py")
library(reticulate)
library(glmnet)
source('/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/HW4.R')
use_python("/Users/bo/.pyenv/versions/3.7.3/lib/python3.7")
reticulate::source_python("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/ridge.py")
reticulate::source_python("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/batch.py")
reticulate::source_python("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/lasso.py")
reticulate::repl_python()
