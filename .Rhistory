glmfo = function(x, y, family, lr, maxit=5000, er = 0.0001){
#initiate betas
beta = rep(0, ncol(x))
#optimize, based on lecture
for (j in 1:maxit) {
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + steps[j] * score
# stop if not updating
if (sum((beta - b_old)^2) < er) {
break
}
}
return(list("coefficients" = beta))
}
glmfo(x,y,family=poisson(link = "log"),lr=lr)
n <- 3000
p <- 4
maxiter <- 500
lr <- rep(1e-3, maxiter)
x <- cbind(1, matrix(runif(n * (p-1)), ncol = p-1))
beta <- c(1, 2, 3, 4)
# poisson family with log link
y = rpois(n,exp(x %*% beta))
maxit = 5000
er = 0.0001
beta = rep(0, ncol(x))
for (j in 1:maxit) {
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + steps[j] * score
# stop if not updating
if (sum((beta - b_old)^2) < er) {
break
}
}
family = poisson(link = "log")
for (j in 1:maxit) {
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + steps[j] * score
# stop if not updating
if (sum((beta - b_old)^2) < er) {
break
}
}
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + steps[j] * score
steps[j]
beta <- beta + steps[j] * score
View(score)
beta <- beta + 0.001 * score
beta = rep(0, ncol(x))
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + 0.001 * score
b_old = beta
mu = family$linkinv(x %*% beta)
n <- 3000
maxiter <- 500
lr <- rep(1e-3, maxiter)
x <- cbind(1, matrix(runif(n * 2), ncol = 2))
beta <- c(1, 0.2, 0.2)
y = rpois(n,exp(x %*% beta))
glmfo = function(x, y, family, lr, maxit=5000, er = 0.0001){
#initiate betas
beta = rep(0, ncol(x))
#optimize, based on lecture
for (j in 1:maxit) {
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + lr[j] * score
# stop if not updating
if (sum((beta - b_old)^2) < er) {
break
}
}
return(list("coefficients" = beta))
}
glmfo(x,y,family = family,lr=lr)
n <- 3000
maxiter <- 500
lr <- rep(1e-3, maxiter)
x <- cbind(1, matrix(runif(n * 2), ncol = 2))
beta <- c(-1, 0.2, 0.2)
y = rpois(n,exp(x %*% beta))
glmfo(x,y,family = family,lr=lr)
beta = rep(0, ncol(x))
b_old = beta
mu = family$linkinv(x %*% beta)
x %*% beta
score = t(x) %*% (y - mu)
beta <- beta + 0.0001 * score
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + 0.0001 * score
sum((beta - b_old)^2)
n <- 3000
maxiter <- 500
lr <- rep(0.0001, maxiter)
x <- cbind(1, matrix(runif(n * 2), ncol = 2))
beta <- c(-1, 0.2, 0.2)
# poisson family with log link
y = rpois(n,exp(x %*% beta))
glmfo = function(x, y, family, lr = 'constant', maxit=5000, er = 0.0001){
if (lr == 'constant'){
lr = rep(0.0001,maxit)
}
#initiate betas
beta = rep(0, ncol(x))
#optimize, based on lecture
for (j in 1:maxit) {
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + lr[j] * score
# stop if not updating
if (sum((beta - b_old)^2) < er) {
break
}
}
return(list("coefficients" = beta))
}
glmfo(x,y,family = family)
n <- 3000
maxiter <- 500
lr <- rep(0.001, maxiter)
x <- cbind(1, matrix(runif(n * 2), ncol = 2))
beta <- c(-1, 0.2, 0.2)
# poisson family with log link
y = rpois(n,exp(x %*% beta))
glmfo = function(x, y, family, lr = 'constant', maxit=5000, er = 0.0001){
if (lr == 'constant'){
lr = rep(0.001,maxit)
}
#initiate betas
beta = rep(0, ncol(x))
#optimize, based on lecture
for (j in 1:maxit) {
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + lr[j] * score
# stop if not updating
if (sum((beta - b_old)^2) < er) {
break
}
}
return(list("coefficients" = beta))
}
glmfo(x,y,family = family)
glmfo = function(x, y, family, lr = 'constant', maxit=10000, er = 0.0001){
if (lr == 'constant'){
lr = rep(0.001,maxit)
}
#initiate betas
beta = rep(0, ncol(x))
#optimize, based on lecture
for (j in 1:maxit) {
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + lr[j] * score
# stop if not updating
if (sum((beta - b_old)^2) < er) {
break
}
}
return(list("coefficients" = beta))
}
glmfo(x,y,family = family)
glmfo = function(x, y, family, lr = 'constant', maxit=10000, er = 1e-10){
if (lr == 'constant'){
lr = rep(0.001,maxit)
}
#initiate betas
beta = rep(0, ncol(x))
#optimize, based on lecture
for (j in 1:maxit) {
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + lr[j] * score
# stop if not updating
if (sum((beta - b_old)^2) < er) {
break
}
}
return(list("coefficients" = beta))
}
glmfo(x,y,family = family)
glmfo = function(x, y, family, lr = 'constant', maxit=10000, er = 1e-10){
if (lr == 'constant'){
lr = rep(0.001,maxit)
}
#initiate betas
beta = rep(0, ncol(x))
#optimize, based on lecture
for (j in 1:maxit) {
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + lr[j] * score
k = sum((beta - b_old)^2)
print(k)
# stop if not updating
if (sum((beta - b_old)^2) < er) {
break
}
}
return(list("coefficients" = beta))
}
glmfo(x,y,family = family)
n <- 3000
maxiter <- 500
lr <- rep(0.001, maxiter)
x <- cbind(1, matrix(runif(n * 2), ncol = 2))
beta <- c(-1, 0.2, 0.2)
y = rpois(n,exp(x %*% beta))
family = poisson(link = "log")
maxit=10000
er = 1e-10
lr = rep(0.001,maxit)
beta = rep(0, ncol(x))
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta + lr[i] * score
beta <- beta +  0.001* score #lr[i]
k = sum((beta - b_old)^2)
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta +  0.001* score #lr[i]
k = sum((beta - b_old)^2)
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta +  0.001* score #lr[i]
k = sum((beta - b_old)^2)
glmfo = function(x, y, family, lr = 'constant', maxit=10000, er = 1e-10){
if (lr == 'constant'){
lr = rep(0.001,maxit)
}
#initiate betas
beta = rep(0, ncol(x))
#optimize, based on lecture
for (i in 1:maxit) {
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta +  0.001* score #lr[i]
k = sum((beta - b_old)^2)
print(k)
# stop if not updating
if (sum((beta - b_old)^2) < er) {
break
}
}
return(list("coefficients" = beta))
}
glmfo(x,y,family=family,maxit=100)
glmfo = function(x, y, family, lr = 'constant', maxit=10000, er = 0.001){
if (lr == 'constant'){
lr = rep(0.001,maxit)
}
#initiate betas
beta = rep(0, ncol(x))
#optimize, based on lecture
for (i in 1:maxit) {
b_old = beta
mu = family$linkinv(x %*% beta)
score = t(x) %*% (y - mu)
beta <- beta +  0.001* score #lr[i]
k = sum((beta - b_old)^2)
print(k)
# stop if not updating
if (sum((beta - b_old)^2) < er) {
break
}
}
return(list("coefficients" = beta))
}
glmfo(x,y,family = family)
?optim
head(iris)
formula = Species ~.
data = iris
df = model.frame(formula,data)
# define the X and Y matrix:
X = model.matrix(formula, df, contrasts.arg = contrasts)
y_name = as.character(formula)[2]
Y = matrix(df[,y_name], ncol = 1)
View(Y)
View(df)
View(X)
View(Y)
class(Y)
class(Y[1])
Y = as.factor(matrix(df[,y_name], ncol = 1))
Y
betas <- matrix(0, nrow = K, ncol = ncol(X)) # betas for each class
probs <- matrix(0, nrow = K, ncol = nrow(X))
K = levels(Y)
K = length(levels(Y))
betas <- matrix(0, nrow = K, ncol = ncol(X)) # betas for each class
probs <- matrix(0, nrow = K, ncol = nrow(X))
n = length(levels(Y))
betas <- matrix(0, nrow = n, ncol = ncol(X)) # betas for each class
probs <- matrix(0, nrow = n, ncol = nrow(X))
i = 1
y.class <- as.numeric(Y == levels(Y)[i])
y.class
beta <- betas[k,]
beta <- betas[i,]
j = 1
p <- 1 / (1 + exp(-X %*% beta))
D <- as.numeric(p * (1 - p))
H <- t(X) %*% diag(D) %*% X
n = 40
er = 0.01
for (i in 1:n) {
# turn y to either 1 or 0
y.class <- as.numeric(Y == levels(Y)[i])
beta <- betas[i,]
#simple logistic regression from lecture
for (j in 1:maxit) {
b_old <- beta
p <- 1 / (1 + exp(-X %*% beta))
D <- as.numeric(p * (1 - p))
H <- t(X) %*% diag(D) %*% X
score <- t(X) %*% (y.class - p)
beta <- beta + (solve(H) %*% score)
if (sum((beta - b_old)^2) < er) {
break
}
}
# Keep the probabilities for each observation for class `k`
probs[i,] <- p
betas[i,] <- beta
}
n = length(levels(Y))
maxit = 40
for (i in 1:n) {
# turn y to either 1 or 0
y.class <- as.numeric(Y == levels(Y)[i])
beta <- betas[i,]
#simple logistic regression from lecture
for (j in 1:maxit) {
b_old <- beta
p <- 1 / (1 + exp(-X %*% beta))
D <- as.numeric(p * (1 - p))
H <- t(X) %*% diag(D) %*% X
score <- t(X) %*% (y.class - p)
beta <- beta + (solve(H) %*% score)
if (sum((beta - b_old)^2) < er) {
break
}
}
# Keep the probabilities for each observation for class `k`
probs[i,] <- p
betas[i,] <- beta
}
pred = apply(probs,2,which.max)
y.pred = levels(Y)[pred]
library('devtools)
)
''
'
library(devtools)
setwd('/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557')
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(reticulate)
install.packages("reticulate")
library(reticulate)
use_python("/Users/bo/.pyenv/versions/3.7.3/lib/python3.7")
setwd("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R")
library(reticulate)
use_python("/Users/bo/.pyenv/versions/3.7.3/lib/python3.7")
setwd("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R")
reticulate::source_python("HW4.py")
library(reticulate)
use_python("/Users/bo/.pyenv/versions/3.7.3/lib/python3.7")
reticulate::source_python("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/ridge.py")
mcars
mtcars
ridge_regressionmpg(mpg ~. mtcars, lambda = 1)
ridge_regression(mpg ~. mtcars, lambda = 1)
library(bis557)
ridge_regression(mpg ~. mtcars, lambda = 1)
ridge_regression(mpg ~. data = mtcars, lambda = 1)
ridge_regression = function(formula, data, lambda = 0) {
rownames = NULL
# Remove Nas
X = model.frame(formula, data)
# Turn into feature matrix
X = model.matrix(formula, X)
# Dependent variable Y:
y_name = as.character(formula)[2]
Y = matrix(data[,y_name], ncol = 1)
# standardization for svd:
X_bar = colMeans(X[, -1])
Y_bar = mean(Y)
X = X[, -1] - rep(X_bar, rep(nrow(X), ncol(X) - 1))
Xscale = drop(rep(1/nrow(X), nrow(X)) %*% X^2)^0.5
X = X/rep(Xscale, rep(nrow(X), ncol(X)))
Y = Y - Y_bar
# initialize beta matrix
coef = matrix(NA_real_, ncol = ncol(X))
# calculate the least squared solution:
svd = svd(X)
coef = svd$v %*% diag(svd$d  / (svd$d^2 + lambda)) %*% t(svd$u) %*% Y
# Combine the coefficients:
scaledcoef = t(as.matrix(coef / Xscale))
intercept <- Y_bar - scaledcoef %*% X_bar
coef <- cbind(intercept, scaledcoef)
# add attributes and class for future use:
coef <- as.vector(coef)
names(coef) <- c("Intercept", colnames(X))
attributes(coef)$formula <- formula
class(coef) <- c(class(coef), "ridge_regression")
return(coef)
}
ridge_regression(mpg ~. data = mtcars, lambda = 1)
ridge_regression(mpg ~., mtcars, lambda = 1)
n = 1000
p = 4
beta = c(1,-1 ,0.1 ,0.5)
X = matrix(rnorm(n*p), nrow=n, ncol = p)
X[,1] =  X[,2]  # make 1st and 2nd columns collinear
y = X %*% beta + rnorm(n) #add noise
sim = cbind(X,y)
View(sim)
#create dataset:
n = 1000
p = 4
beta = c(1,-1 ,0.1 ,0.5)
X = matrix(rnorm(n*p), nrow=n, ncol = p)
X[,1] =  X[,2]  # make 1st and 2nd columns collinear
y = X %*% beta + rnorm(n) #add noise
sim = cbind(X,y)
ridge_regression(V5 ~., sim, lambda = 1)
n = 1000
p = 4
beta = c(1,-1 ,0.1 ,0.5)
X = matrix(rnorm(n*p), nrow=n, ncol = p)
X[,1] =  X[,2]  # make 1st and 2nd columns collinear
y = X %*% beta + rnorm(n) #add noise
sim = as.data.frame(cbind(X,y))
View(sim)
ridge_regression(V5 ~., sim, lambda = 1)
#create dataset:
n = 1000
p = 4
beta = c(1,-1 ,0.1 ,0.5)
X = matrix(rnorm(n*p), nrow=n, ncol = p)
X[,1] = X[,1]*0.05 + X[,2]*0.95
y = X %*% beta + rnorm(n) #add noise
sim = as.data.frame(cbind(X,y))
ridge_regression(V5 ~., sim, lambda = 1)
#create dataset:
n = 1000
p = 4
beta = c(0.2,-0.2 ,0.1 ,0.5)
X = matrix(rnorm(n*p), nrow=n, ncol = p)
X[,1] = X[,1]*0.05 + X[,2]*0.95
y = X %*% beta + rnorm(n) #add noise
sim = as.data.frame(cbind(X,y))
ridge_regression(V5 ~., sim, lambda = 1)
#create dataset:
beta = c(0.2,-0.2 ,0.1 ,0.5)
X = matrix(rnorm(1000*4), nrow=1000, ncol = 4)
X[,1] = X[,2]
y = X %*% beta + rnorm(n) #add noise
sim = as.data.frame(cbind(X,y))
#run ridge regression:
ridge_regression(V5 ~., sim, lambda = 1)
#create dataset:
beta = c(0.2,-0.2 ,0.1 ,0.5)
X = matrix(rnorm(1000*4), nrow=1000, ncol = 4)
X[,1] = 2*X[,2]
y = X %*% beta + rnorm(n) #add noise
sim = as.data.frame(cbind(X,y))
#run ridge regression:
ridge_regression(V5 ~., sim, lambda = 1)
#create dataset:
beta = c(0.2,-0.2 ,0.1 ,0.5)
X = matrix(rnorm(1000*4), nrow=1000, ncol = 4)
X[,1] = 2*X[,2] + X[,1]
y = X %*% beta + rnorm(n) #add noise
sim = as.data.frame(cbind(X,y))
#run ridge regression:
ridge_regression(V5 ~., sim, lambda = 1)
library(reticulate)
library(bis557)
use_python("/Users/bo/.pyenv/versions/3.7.3/lib/python3.7")
reticulate::source_python("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/ridge.py")
library(reticulate)
library(bis557)
use_python("/Users/bo/.pyenv/versions/3.7.3/lib/python3.7")
reticulate::source_python("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/R/ridge.py")
