loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
increment = update(beta)
beta = (beta - lambda*increment )
loss(X,Y,beta)
gradient_descent = function(X, Y, lambda = 0.0000001, N = 5000 ){
# add intercept term:
X = as.matrix(cbind(1,X))
# initialize the betas at all 0s:
beta = as.matrix(rep(0,length(X[1,])))
# calculate the gradient function:
update = function(beta){
increment = (2 * t(X) %*% X %*% beta - 2 * t(X) %*% Y)
return(increment)
}
# monitor the loss function:
loss = function(beta){
pred.y = X %*% beta
loss = sum((pred.y - Y)^2)
return(loss)
}
for (i in 1:N){
beta = beta - update(beta)
los = loss(beta)
# monitor the process:
print(paste("Epoch number:", i,"Current Loss is:", los))
}
}
gradient_descent(X,Y)
X = as.matrix(cbind(1,X))
drop(X[,1])
X = X[,-1]
head(X)
beta = as.matrix(rep(0,length(X[1,])))
update = function(beta){
increment = (2 * t(X) %*% X %*% beta - 2 * t(X) %*% Y)
return(increment)
}
loss = function(beta){
pred.y = X %*% beta
loss = sum((pred.y - Y)^2)
return(loss)
}
beta = beta - update(beta)
los = loss(beta)
k = 100 * i
gradient_descent = function(X, Y, lambda = 0.0000001, N = 5000 ){
# add intercept term:
X = as.matrix(cbind(1,X))
# initialize the betas at all 0s:
beta = as.matrix(rep(0,length(X[1,])))
# calculate the gradient function:
update = function(beta){
increment = (2 * t(X) %*% X %*% beta - 2 * t(X) %*% Y)
return(increment)
}
# monitor the loss function:
loss = function(beta){
pred.y = X %*% beta
loss = sum((pred.y - Y)^2)
return(loss)
}
for (i in 1:N){
beta = beta - update(beta)
los = loss(beta)
k = 100 * i
# monitor the process:
print(paste("Epoch number per 100 iterations:", k,"Current Loss is:", los))
}
}
gradient_descent(X,Y)
gradient_descent(X,Y,N=10)
gradient_descent = function(X, Y, lambda = 0.00000001, N = 5000 ){
# add intercept term:
X = as.matrix(cbind(1,X))
# initialize the betas at all 0s:
beta = as.matrix(rep(0,length(X[1,])))
# calculate the gradient function:
update = function(beta){
increment = (2 * t(X) %*% X %*% beta - 2 * t(X) %*% Y)
return(increment)
}
# monitor the loss function:
loss = function(beta){
pred.y = X %*% beta
loss = sum((pred.y - Y)^2)
return(loss)
}
for (i in 1:N){
beta = beta - update(beta)
los = loss(beta)
# monitor the process:
print(paste("Epoch number:", i,"Current Loss is:", los))
}
}
gradient_descent(X,Y,N=10)
gradient_descent = function(X, Y, lambda = 0.00000001, N = 5000 ){
# add intercept term:
X = as.matrix(cbind(1,X))
# initialize the betas at all 0s:
beta = as.matrix(rep(0,length(X[1,])))
# calculate the gradient function:
update = function(beta){
increment = (2 * t(X) %*% X %*% beta - 2 * t(X) %*% Y)
return(increment)
}
# monitor the loss function:
loss = function(beta){
pred.y = X %*% beta
loss = sum((pred.y - Y)^2)
return(loss)
}
for (i in 1:N){
beta = beta - lambda*update(beta)
los = loss(beta)
# monitor the process:
print(paste("Epoch number:", i,"Current Loss is:", los))
}
}
gradient_descent(X,Y,N=10)
gradient_descent(X,Y)
gradient_descent(X,Y,N=20)
gradient_descent = function(X, Y, lambda = 0.00000001, N = 5000 ){
# add intercept term:
X = as.matrix(cbind(1,X))
# initialize the betas at all 0s:
beta = as.matrix(rep(0,length(X[1,])))
# calculate the gradient function:
update = function(beta){
increment = (2 * t(X) %*% X %*% beta - 2 * t(X) %*% Y)
return(increment)
}
# monitor the loss function:
loss = function(beta){
pred.y = X %*% beta
loss = sum((pred.y - Y)^2)
return(loss)
}
for (i in 1:N){
beta = beta - lambda*update(beta)
los = loss(beta)
# monitor the process:
print(paste("Epoch number:", i,"   Current Loss is:", los))
}
}
gradient_descent(X,Y)
gradient_descent = function(X, Y, lambda = 0.000001, N = 5000 ){
# add intercept term:
X = as.matrix(cbind(1,X))
# initialize the betas at all 0s:
beta = as.matrix(rep(0,length(X[1,])))
# calculate the gradient function:
update = function(beta){
increment = (2 * t(X) %*% X %*% beta - 2 * t(X) %*% Y)
return(increment)
}
# monitor the loss function:
loss = function(beta){
pred.y = X %*% beta
loss = sum((pred.y - Y)^2)
return(loss)
}
for (i in 1:N){
beta = beta - lambda*update(beta)
los = loss(beta)
# monitor the process:
print(paste("Epoch number:", i,"   Current Loss is:", los))
}
return(beta)
}
X = as.matrix(runif(9),c(3,3))
print(X)
as.matrix()
?as.matrix
X = as.matrix(runif(9),nrow = 3, ncol = 3)
Y = as.matrix(c(1,2,3))
X = as.matrix(runif(9),nrow = 3, ncol = 3)
Y = as.matrix(c(1,2,3))
beta = solve(t(X) %*% X) %*% (t(X) %*% Y)
X = as.matrix(runif(9),nrow = 3, ncol = 3)
dim(X)
X = matrix(runif(9),nrow = 3, ncol = 3)
Y = as.matrix(c(1,2,3))
beta = solve(t(X) %*% X) %*% (t(X) %*% Y)
beta
beta1 = solve(t(X) %*% X) %*% (t(X) %*% Y)
gradient_descent(X,Y)
gradient_descent = function(X, Y, lambda = 0.000001, N = 5000 ){
# Do we need to add intercept term?
# add intercept term:
# X = as.matrix(cbind(1,X))
# initialize the betas at all 0s:
beta = as.matrix(rep(0,length(X[1,])))
# calculate the gradient function:
update = function(beta){
increment = (2 * t(X) %*% X %*% beta - 2 * t(X) %*% Y)
return(increment)
}
# monitor the loss function:
loss = function(beta){
pred.y = X %*% beta
loss = sum((pred.y - Y)^2)
return(loss)
}
for (i in 1:N){
beta = beta - lambda*update(beta)
los = loss(beta)
# monitor the process:
print(paste("Epoch number:", i,"   Current Loss is:", los))
}
return(beta)
}
gradient_descent(X,Y)
X = matrix(runif(9),nrow = 3, ncol = 3)
Y = as.matrix(c(1,2,3))
beta1 = solve(t(X) %*% X) %*% (t(X) %*% Y)
gradient_descent(X,Y)
gradient_descent(X,Y,N=10)
gradient_descent(X,Y,N=9)
gradient_descent(X,Y,N=5)
gradient_descent(X,Y,N=4)
gradient_descent = function(X, Y, lambda = 0.001, N = 5000 ){
# Do we need to add intercept term?
# add intercept term:
# X = as.matrix(cbind(1,X))
# initialize the betas at all 0s:
beta = as.matrix(rep(0,length(X[1,])))
# calculate the gradient function:
update = function(beta){
increment = (2 * t(X) %*% X %*% beta - 2 * t(X) %*% Y)
return(increment)
}
# monitor the loss function:
loss = function(beta){
pred.y = X %*% beta
loss = sum((pred.y - Y)^2)
return(loss)
}
for (i in 1:N){
beta = beta - lambda*update(beta)
los = loss(beta)
# monitor the process:
print(paste("Epoch number:", i,"   Current Loss is:", los))
}
return(beta)
}
gradient_descent(X,Y)
beta2 = gradient_descent(X,Y)
X %*% beta2
tes()
test()
library('covr')
report()
document()
library(bis557)
?gradient_descent()
package_coverage()
check()
report()
document()
report()
document()
report(0)
report()
test_dir
test_dir()
test_dir("/Users/bo/Desktop/2020 fall/Computational/homework-1/bis557/tests")
report()
test()
use_testthat()
test()
report()
?expect_less_than
document()
check(0)
test()
test()
test()
test()
report()
test()
test()
test()
document()
check()
document()
check()
report()
use_travis()
library(bis557)
data(iris)
fit_linear_model <- linear_model(Sepal.Length ~ ., iris)
print(fit_linear_model)
# create the feature vectors X and the label vectors Y:
X = matrix(runif(9),nrow = 3, ncol = 3)
Y = as.matrix(c(1,2,3))
#estimate beta from gradient descent:
beta = as.matrix(gradient_descent(X,Y, N=10000))
#print the loss after 10K iterations:
loss = sum((Y - X %*% beta)^2)
print(loss)
document()
build_readme()
build_readme()
build_readme()
code_coverage()
covr::codecov()
use_coverage()
codecov(token = "e898961e-48f2-4d88-835e-d9f28be44fef")
document()
build_readme()
build_readme()
use_coverage()
build_readme()
build_readme()
